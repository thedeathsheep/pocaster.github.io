---
layout: post
title: AI学习笔记4
cover-img: /assets/img/0028963732_0.jpg
thumbnail-img: /assets/img/0028963732_0.jpg
share-img: /assets/img/0028963732_0.jpg
tags: [Public, AI]
author: pocaster
published: false
math: true
mermaid: true
---

### **自注意力机制（Self-Attention）通俗解释**
自注意力是 Transformer 的核心技术，让模型能**动态关注输入的不同部分**，从而理解上下文关系。下面用最直观的方式说明它的工作原理。

---

## **1. 核心思想：动态权重分配**
假设我们要翻译这句话：
> **“猫吃鱼”**

传统方法（如RNN）会按顺序处理每个词，但**自注意力**让模型同时看所有词，并计算它们之间的关联强度（权重）。
- **“猫”** 和 **“吃”** 关系强（因为猫是吃的执行者）。
- **“吃”** 和 **“鱼”** 关系强（因为鱼是被吃的对象）。

自注意力会为这些关系分配不同的权重，类似人类阅读时“画重点”。

---

## **2. 具体步骤（简化版）**
### **① 输入表示**
每个词（如“猫”）被转换成向量（比如 `[0.2, -0.5, 1.3]`），称为 **词嵌入（Embedding）**。

### **② 生成 Q, K, V 向量**
对每个词，计算三个向量：
- **Query（Q）**：当前词的“提问”（想知道其他词和它的关系）。
- **Key（K）**：其他词的“标签”（用来匹配 Query）。
- **Value（V）**：实际提供的信息（加权求和的原料）。

*（通过矩阵乘法生成，参数在训练中学习）*

### **③ 计算注意力分数**
用 `Q` 和所有 `K` 做点积（相似度计算），得到分数（分数越高，关系越强）。
例如：
- `Q("猫") · K("吃") = 8.2`（高分，强关联）
- `Q("猫") · K("鱼") = 0.5`（低分，弱关联）

### **④ Softmax 归一化**
将分数转换为概率分布（权重总和=1），例如：
- “猫”对“吃”的权重：0.7
- “猫”对“鱼”的权重：0.3

### **⑤ 加权求和**
用权重对 `V` 加权求和，得到最终表示：
`新“猫” = 0.7 * V("吃") + 0.3 * V("鱼")`

这样，“猫”的向量就包含了上下文信息（知道它和“吃”关系密切）。

---

## **3. 多头注意力（Multi-Head Attention）**
- **动机**：单组注意力可能忽略不同角度的关系（比如语法 vs 语义）。
- **做法**：
  并行多组独立的 Q/K/V 计算（比如8组），每组关注不同模式，最后拼接结果。
  *（类似用多台显微镜从不同角度观察样本）*

---

## **4. 为什么比 RNN/CNN 强？**

| 方法          | 缺点                          | 自注意力的优势               |
|---------------|-----------------------------|---------------------------|
| **RNN**       | 必须顺序计算，难以捕捉长距离依赖 | 直接计算所有词的关系，并行高效 |
| **CNN**       | 局部窗口，无法全局关联          | 任意距离的词都能直接交互      |

---

## **5. 举个实际例子**
**句子**：“The animal didn't cross the street because it was too tired.”
**问题**：“it”指代什么？（animal 还是 street？）

自注意力会：
1. 计算 `Q("it")` 和所有 `K` 的分数。
2. 发现 `K("animal")` 分数最高（因为“it”和“animal”在语义和语法上更匹配）。
3. 输出指代“animal”。

---

## **6. 数学公式（可选）**
自注意力的计算如下：
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
- \(d_k\) 是向量的维度，用于缩放点积结果（防止梯度消失）。

---

### **一句话总结**
自注意力让模型动态计算每个词与其他词的关系权重，通过加权求和融合上下文信息，从而解决长距离依赖问题，成为 Transformer 的“灵魂”。



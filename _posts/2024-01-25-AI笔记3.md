---
layout: post
title: "AI学习笔记3"
date: 2024-01-25 10:00:00 +0800
tags: [Public, AI]
excerpt: "本篇笔记系统梳理了迁移学习与Transformer的核心原理、典型应用与代码实践，适合AI初学者和进阶者参考。"
cover-img: /assets/img/0028963732_0.jpg
thumbnail-img: /assets/img/0028963732_0.jpg
share-img: /assets/img/0028963732_0.jpg
author: pocaster
published: false
math: true
mermaid: true
---

## 什么是迁移学习？

迁移学习（Transfer Learning）是机器学习中的一种方法，其核心思想是**将从一个任务（或领域）中学到的知识迁移到另一个相关任务上**，从而减少对新任务的训练数据需求或提升模型性能。迁移学习中的"迁移模型"通常指**预训练模型（Pre-trained Model）**，即在源任务上训练好的模型，通过调整（如微调）应用于目标任务。

---

### 1. 迁移学习的核心思想

- **传统机器学习**：每个任务独立训练模型，数据与任务强绑定。

- **迁移学习**：利用源任务（Source Task）的知识（如特征、模型参数）帮助目标任务（Target Task）。
  - **前提假设**：源任务和目标任务之间存在一定的相关性（如共享底层特征、语义模式）。

---

### 2. 为什么需要迁移模型？

- **数据稀缺**：目标任务标注数据少（如医疗影像标注成本高）。

- **训练成本高**：从零训练大模型（如GPT、ResNet）需要海量数据和算力。

- **泛化性提升**：源任务学到的通用特征（如边缘、纹理、语法）可复用。

---

### 3. 迁移模型的典型场景

#### ① 计算机视觉（CV）

- **预训练模型**：在ImageNet（百万级图像分类任务）上训练的ResNet、VGG。

- **迁移到小数据任务**：
  - 固定特征提取器：用ResNet提取图像特征，接一个简单的分类器（如花卉分类）。
  - 微调（Fine-tuning）：解冻部分层，用目标数据调整参数。

#### ② 自然语言处理（NLP）

- **预训练模型**：BERT、GPT（在通用语料上训练，学习语言表示）。

- **迁移到下游任务**：
  - 文本分类：在BERT后加全连接层，用领域数据微调。
  - 问答系统：复用BERT的编码能力，调整输出层。

#### ③ 跨领域迁移

- 从**新闻情感分析**模型迁移到**商品评论情感分析**。
- 从**英语NLP模型**迁移到**小语种**（通过共享词嵌入或跨语言模型如mBERT）。

---

### 4. 迁移学习的关键技术

#### ① 特征迁移（Feature Transfer）

- 使用源模型提取的特征作为目标模型的输入。
- 例如：用ImageNet预训练的CNN提取图像特征，再用SVM分类。

#### ② 参数迁移（Parameter Transfer）

- 将源模型的参数作为目标模型的初始化，进一步微调。
- 例如：BERT在特定领域（如医学文本）上继续训练。

#### ③ 冻结与微调（Freezing & Fine-tuning）

- **冻结**：固定源模型的底层（学习通用特征的部分，如CNN的浅层卷积核）。
- **微调**：调整顶层（任务相关部分，如分类层）。

#### ④ 领域适应（Domain Adaptation）

- 当源数据和目标数据分布不同时（如合成图像→真实图像），通过对抗训练（GAN）或特征对齐减少差异。

---

### 5. 代码示例：迁移模型实战

#### 案例：用预训练ResNet进行猫狗分类（少量数据）

```python
from torchvision import models, transforms
import torch.nn as nn

# 加载预训练ResNet，冻结底层参数
model = models.resnet18(pretrained=True)
for param in model.parameters():
    param.requires_grad = False  # 冻结参数

# 替换顶层分类器（适应新任务）
model.fc = nn.Linear(512, 2)  # 输出2类（猫/狗）

# 微调（仅训练顶层）
optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)
```

#### 案例：用BERT进行文本分类

```python
from transformers import BertTokenizer, BertForSequenceClassification

# 加载预训练BERT
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# 微调（示例代码片段）
inputs = tokenizer("This movie is great!", return_tensors="pt")
outputs = model(**inputs, labels=torch.tensor([1]))  # 1=正面
```

---

### 6. 迁移学习的优势与挑战

- **优势**：
  - 减少数据需求，提升小数据场景性能。
  - 加速训练（无需从零开始）。

- **挑战**：
  - **负迁移**：源任务与目标任务不相关时，性能反而下降。
  - 领域差异过大时需额外适配（如领域对抗训练）。

---

### 总结

- **迁移模型** = 预训练模型 + 目标适配（微调/特征提取）。
- **本质**：通过参数共享或特征复用，将通用知识迁移到特定任务。
- **典型模型**：CV中的ResNet、NLP中的BERT、多模态中的CLIP等。

迁移学习已成为现代AI的核心技术，尤其在数据稀缺的领域（如医疗、金融）表现突出。

## Transformer 基础原理

Transformer 是一种基于**自注意力机制（Self-Attention）**的深度学习架构，由 Google 团队在 2017 年的论文《[Attention Is All You Need](https://arxiv.org/abs/1706.03762)》中首次提出。它彻底改变了自然语言处理（NLP）领域，并逐渐扩展到计算机视觉（CV）、语音识别等多模态任务。其核心思想是**完全依赖注意力机制**（而非传统的循环或卷积结构）来建模序列数据中的长距离依赖关系。

---

### 1. Transformer 的核心设计

#### （1）自注意力机制（Self-Attention）

- **核心公式：**

  $$
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
  $$

- **输入**：查询（Query）、键（Key）、值（Value）矩阵，均来自同一输入序列的线性变换。

- **作用**：计算序列中每个位置与其他位置的关联权重（注意力分数），加权聚合值（Value）信息。

- **优势**：直接建模任意两个位置的关系，无论距离多远（解决 RNN 的长距离依赖问题）。

#### （2）多头注意力（Multi-Head Attention）

- 并行运行多组自注意力机制，捕捉不同子空间的语义模式（如语法、语义、指代关系）。

- **公式：**

  $$
  \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
  $$
  其中，$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$。

#### （3）位置编码（Positional Encoding）

- Transformer 本身无时序概念，需通过**位置编码**注入序列顺序信息：

  $$
  PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{\text{model}}})
  $$
  $$
  PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d_{\text{model}}})
  $$
  - 其中，$pos$ 是位置，$i$ 是维度索引。

#### （4）残差连接与层归一化

- 每个子层（注意力、前馈网络）后接**残差连接**和**层归一化（LayerNorm）**，缓解梯度消失：

  $$
  \text{LayerNorm}(x + \text{Sublayer}(x))
  $$

#### （5）前馈网络（Feed-Forward Network, FFN）

- 每个位置的独立全连接层，增强非线性：

  $$
  \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
  $$

---

### 2. Transformer 的架构组成

#### （1）编码器（Encoder）

- 由 $N$ 个相同层堆叠而成，每层包含：

  1. **多头自注意力**（捕捉全局依赖）
  2. **前馈网络**（逐位置变换）
  3. **残差连接 + 层归一化**

#### （2）解码器（Decoder）

- 比编码器多一个**掩码多头注意力**（Masked Multi-Head Attention）：
  - **掩码机制**：防止解码时看到未来信息（确保自回归生成）。
- 解码器还会接收编码器的输出作为键和值（Encoder-Decoder Attention）。

<center>
  <img src="https://miro.medium.com/v2/resize:fit:1400/1*BHzGVskWGS_3jEcYYi6miQ.png" alt="Transformer结构示意图" style="max-width:100%;">
  <br>
  Transformer结构示意图
</center>

---

### 3. Transformer 的优势

1. **并行计算**：自注意力可并行处理所有位置，远快于 RNN 的序列计算。

2. **长距离依赖**：直接建模任意位置关系，避免 RNN 的梯度消失。

3. **可扩展性**：通过堆叠层数捕捉多层次特征（如词→短语→句子级语义）。

---

### 4. Transformer 的变体与应用

#### （1）NLP 领域

- **BERT**：仅用编码器，双向上下文建模（掩码语言模型）。
- **GPT**：仅用解码器，自回归生成文本。
- **T5**：编码器-解码器结构，统一文本生成框架。

#### （2）跨模态领域

- **ViT**（Vision Transformer）：将图像分块为序列，用 Transformer 处理。
- **CLIP**：联合训练图像和文本编码器，实现跨模态检索。

#### （3）高效改进

- **稀疏注意力**（如 Longformer、BigBird）：降低计算复杂度。
- **蒸馏/量化**（如 DistilBERT）：压缩模型大小。

---

### 5. 关键总结

- **核心创新**：自注意力机制替代循环/卷积，实现全局依赖建模。
- **核心组件**：多头注意力、位置编码、残差连接、层归一化。
- **应用场景**：从 NLP（BERT、GPT）到 CV（ViT）、多模态（CLIP）。
- **缺陷与改进**：计算复杂度高（$O(n^2)$），催生稀疏注意力、低秩近似等方法。

Transformer 的提出标志着深度学习从"局部感知"迈向"全局理解"，成为现代 AI 模型的基石架构。

---

## Transformer 通俗解释

我用最简单的方式解释 **Transformer**，就像讲故事一样：

---

### 1. Transformer 是干什么的？

- **任务**：处理序列数据（比如一句话、一段音频、一串时间序列）。

- **目标**：让模型理解序列中每个部分和其他部分的关联（比如"猫"和"抓"的关系）。

---

### 2. 核心思想：自注意力（Self-Attention）

- **比喻**：读一句话时，你的大脑会自动关注重点词。

  - 比如："**猫**抓**老鼠**"→ 模型会计算"猫"和"抓"、"老鼠"的关联强度（权重）。

- **怎么算？**

  1. 每个词生成3个向量：**Query（问题）**、**Key（钥匙）**、**Value（答案）**。
  2. 用`Query`和`Key`计算相似度（注意力分数），决定`Value`的权重。
  3. 加权求和得到新表示（相当于"综合上下文后的词向量"）。

---

### 3. 为什么不用 RNN 或 CNN？

- **RNN**：像接力赛，必须按顺序处理，慢且容易忘掉开头。

- **CNN**：像局部放大镜，只能看附近几个词。

- **Transformer**：像上帝视角，一眼看完所有词的关系，还能并行计算（更快！）。

---

### 4. 关键组件

#### ① 多头注意力（Multi-Head Attention）

- **比喻**：用多组"眼睛"从不同角度分析句子。

  - 比如：一组看语法，一组看语义，一组看指代关系。

- **效果**：模型更全面，避免片面理解。

#### ② 位置编码（Positional Encoding）

- **问题**：Transformer 本身不知道词的顺序（"猫抓老鼠"和"老鼠抓猫"对它没区别）。

- **解决**：给每个词加一个"位置编号"（类似坐标），告诉模型谁在前谁在后。

#### ③ 残差连接 + 层归一化

- **作用**：防止模型训练时"学崩了"，像给学习加个安全绳。

---

### 5. 典型结构（编码器-解码器）

- **编码器（Encoder）**：理解输入（比如把句子变成"机器能懂的密码"）。

- **解码器（Decoder）**：生成输出（比如把"密码"翻译成另一种语言）。

- **关键区别**：解码器在生成时会遮住后面的词（防止作弊，像考试时不能看答案）。

---

### 6. 为什么火？

- **优点**：
  - 比 RNN/CNN 更擅长长文本（比如理解整篇文章）。
  - 训练快（所有词一起算，不用排队）。

- **代表作**：
  - **BERT**（编码器，擅长理解语言，比如问答）。
  - **GPT**（解码器，擅长生成文本，比如写诗）。
  - **ViT**（处理图片，把图像拆成"词"一样的小块）。

---

## 参考资料

- Vaswani, A., et al. (2017). Attention Is All You Need. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
- 相关BERT、GPT、ViT等官方文档

## 非Transformer架构综述

近年来，除了Transformer架构，AI领域涌现出一批高效的新型神经网络结构，代表性方案包括Liquid AI的液态神经网络、RWKV、Mamba和RetNet等。这些架构在内存效率、推理速度和多模态处理等方面展现出独特优势，成为Transformer的重要补充和潜在替代者。

### 1. Liquid AI与液态神经网络

- **背景**：Liquid AI由MIT CSAIL前研究人员创立，推出了基于第一性原理的Liquid Foundation Models（LFM）。
- **核心特点**：
  - **高效内存**：相比Transformer，LFM在处理长序列时内存占用更低。
  - **多模态支持**：可处理视频、音频、文本、时间序列等多种数据类型。
  - **实时适应**：保留液态神经网络的动态调整能力，推理过程中可实时自适应。
- **性能表现**：LFM-1B、LFM-3B、LFM-40B等模型在多个基准测试中实现SOTA，尤其在内存和推理效率上表现突出。例如，LFM-3B仅需16GB内存即可媲美同规模Transformer。

### 2. 其他非Transformer架构

- **RWKV**：
  - 国产开源大语言模型，采用线性注意力机制，融合RNN与Transformer优点。
  - 内存和计算需求随序列长度线性增长，远优于Transformer的二次方复杂度。
- **Mamba**：
  - 新一代SSM（状态空间模型）架构，完全基于循环结构，无需注意力机制。
  - 具备线性扩展能力，推理吞吐量高，适合长序列建模。
- **RetNet**：
  - 采用多尺度保留机制替代多头注意力，推理成本与序列长度无关。
  - 在解码速度和内存节省方面表现优异。

### 3. 未来展望

尽管非Transformer架构在效率和资源消耗方面展现出巨大潜力，但其在大规模模型上的通用性和性能仍需进一步验证。Transformer依然是当前主流，但随着技术进步，未来有望出现更多高效、可扩展的非Transformer模型，推动AI基础架构的多样化发展。